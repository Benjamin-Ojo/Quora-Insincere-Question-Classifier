{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Quora Insincere Question Classifier**\n",
        "---"
      ],
      "metadata": {
        "id": "a8yvK-qVHV_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction** \n",
        "--- \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tw6sZwmhHV_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background: "
      ],
      "metadata": {
        "id": "UyICewq0HV_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's digital age, social platforms have become hubs for information sharing and community engagement. Quora, one such platform, provides users with a platform to ask questions and receive answers from a diverse community. However, as with any open forum, there is a potential for misuse, where users may pose insincere or deceptive questions.\n",
        "\n",
        "The classification of insincere questions is a significant challenge in natural language processing (NLP). It requires the ability to discern the underlying intent and identify questions that may be misleading, inflammatory, or offensive. Accurately detecting and categorizing these insincere questions is crucial to maintaining the quality and credibility of a platform like Quora.\n",
        "\n",
        "In this project, we delve into the task of insincere question classification on Quora, using machine learning and NLP techniques. Our objective is to develop a robust and efficient model that can automatically differentiate between sincere and insincere questions."
      ],
      "metadata": {
        "id": "fqudzesOHV_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset:"
      ],
      "metadata": {
        "id": "YrfmQsCNHV_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our insincerity quest, we will leverage the Quora Insincere Questions Classification dataset, which is publicly available on Kaggle. This dataset comprises a large collection of questions from Quora, along with corresponding labels indicating whether each question is sincere or insincere. The dataset is annotated by human reviewers, providing valuable ground truth for training and evaluation purposes.\n",
        "\n",
        "Our dataset are divided into training and testing dataset.The data contains the following columns: \n",
        "\n",
        "1. **qid**: This is a unique number for each of the question in our datasets. \n",
        "2. **question_text**: The full text of a Quora question. \n",
        "3. **target**: The label encoding on whether a question is sincere or not. "
      ],
      "metadata": {
        "id": "rU_W_4aSHV_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approachs: "
      ],
      "metadata": {
        "id": "VSHUko1xHV_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tackle this problem, we will adopt a supervised learning approach. We will explore various NLP techniques to build a classification model that can effectively distinguish between sincere and insincere questions. This will involve several key steps:\n",
        "\n",
        "1. Data Collection: The Quora Insincere Questions Classification dataset is collected and downloaded from Kaggle. The dataset will be imported and processed using Google Colab. The data structures and features will be explored to gain a better understanding of the dataset.\n",
        "\n",
        "2. Data Preprocessing: Text data is preprocessed by tokenization, lowercasing, and removal of stop words and punctuation. Techniques like stemming or lemmatization may be applied for further normalization.\n",
        "\n",
        "3. Feature Extraction: The preprocessed text data will be transformed into numerical representations suitable for machine learning algorithms. For this project, we will be using word embeddings, such as Word2Vec or GloVe, to convert the text into dense vector representations that capture semantic relationships between words.\n",
        "\n",
        "4. Model Selection and Training: For this project, we will explore various NLP models suitable for insincere question classification, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), or transformer models like BERT. These models have shown promising results in NLP tasks and can capture complex patterns and dependencies in text data. We will select the most appropriate model based on its performance on the validation dataset and train it using the labeled training dataset.\n",
        "\n",
        "5. Model Evaluation: The trained NLP model will be evaluated using appropriate evaluation metrics, such as accuracy, precision, recall, and F1-score. The performance of the model will be assessed on the test dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "VVT3YeePHV_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages:"
      ],
      "metadata": {
        "id": "wqc6vkuWHV_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipution packages. \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization packages.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File manager packages.\n",
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "\n",
        "# Tensoflow packages.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Flatten, Conv1D, MaxPooling1D, Bidirectional, LSTM, RNN, GRU, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "# Other packages.\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2023-05-24T13:43:10.570146Z",
          "iopub.execute_input": "2023-05-24T13:43:10.570721Z",
          "iopub.status.idle": "2023-05-24T13:43:10.582346Z",
          "shell.execute_reply.started": "2023-05-24T13:43:10.570676Z",
          "shell.execute_reply": "2023-05-24T13:43:10.580989Z"
        },
        "trusted": true,
        "id": "d_eOWoDkHV_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Collecion**\n",
        "---"
      ],
      "metadata": {
        "id": "CEy31oLBHV_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project we will be working from the kaggle notebook,and since our data is already in our working station we would just be loading our dataset from the kaggle input directory. \n",
        "\n",
        "To download this dataset from kaggle for colab and local system use, i will be providing commented code to help with this."
      ],
      "metadata": {
        "id": "AYJoZmTxHV_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle Data Import:"
      ],
      "metadata": {
        "id": "NpzsFgpDHV_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This commented code below is only for colab notebooks."
      ],
      "metadata": {
        "id": "JqEHxYTmHV_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining data directory.\n",
        "!mkdir '1. Dataset'"
      ],
      "metadata": {
        "id": "gVOwQ1rbJcTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install kaggle api with Pip. \n",
        "!pip install kaggle"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T13:11:32.229951Z",
          "iopub.execute_input": "2023-05-24T13:11:32.230471Z",
          "iopub.status.idle": "2023-05-24T13:11:32.236433Z",
          "shell.execute_reply.started": "2023-05-24T13:11:32.230436Z",
          "shell.execute_reply": "2023-05-24T13:11:32.235332Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nmHJhrDHV_X",
        "outputId": "c525fc42-d217-484b-903f-c11d123f860f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading Kaggle api token key.\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "xuApqp3gIEs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing api token location.\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQlixkfIIKfD",
        "outputId": "fddb32bf-e76b-42d8-f2f3-ab26036e3cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äò/root/.kaggle‚Äô: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the appropriate permissions \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# verify api key.\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vws08bD6IRky",
        "outputId": "e04e7e21-58bb-48d9-8bff-ed2f6b19695c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                       title                                          size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------  --------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "arnabchaki/data-science-salaries-2023                     Data Science Salaries 2023 üí∏                   25KB  2023-04-13 09:55:16          19436        545  1.0              \n",
            "fatihb/coffee-quality-data-cqi                            Coffee Quality Data (CQI May-2023)             22KB  2023-05-12 13:06:39           1414         43  1.0              \n",
            "ashpalsingh1525/imdb-movies-dataset                       IMDB movies dataset                             3MB  2023-04-28 23:18:15           2103         45  1.0              \n",
            "iammustafatz/diabetes-prediction-dataset                  Diabetes prediction dataset                   734KB  2023-04-08 06:11:45          10103        145  1.0              \n",
            "utkarshx27/inflation-rate-in-asia                         Inflation Rate in Asia                          3KB  2023-05-13 17:41:29            799         29  1.0              \n",
            "radheshyamkollipara/bank-customer-churn                   Bank Customer Churn                           307KB  2023-04-28 16:32:01           2023         34  1.0              \n",
            "desalegngeb/students-exam-scores                          Students Exam Scores: Extended Dataset        695KB  2023-04-14 00:15:38           7996        155  1.0              \n",
            "chitrakumari25/corona-virus-latest-data-2023              Corona virus latest data 2023                  10KB  2023-04-29 16:00:51           2111         53  1.0              \n",
            "omarsobhy14/university-students-complaints-and-reports    University Students Complaints & Reportsüìùüë®‚Äçüéì   38KB  2023-05-12 19:46:45            729         30  1.0              \n",
            "utkarshx27/starbucks-nutrition                            Starbucks Nutrition Facts                       2KB  2023-05-10 05:42:59           1517         44  1.0              \n",
            "utkarshx27/survey-of-labour-and-income-dynamics           Survey of Labour and Income Dynamics           54KB  2023-05-07 08:00:47            587         21  1.0              \n",
            "rajkumarpandey02/rainfall-in-all-india-dataset-1901-2016  Rainfall in All India Dataset 1901-2016         3KB  2023-05-18 05:45:59            391         28  1.0              \n",
            "faisaljanjua0555/best-video-games-of-all-time             Best Video Games of All Time                   24KB  2023-05-14 19:44:10            799         35  0.7058824        \n",
            "utkarshx27/world-gdp-growth-1980-2028                     World GDP growth 1980-2028                     53KB  2023-05-13 17:54:00            911         31  0.88235295       \n",
            "utkarshx27/global-poverty-and-inequality-dataset          Global poverty and inequality dataset           4MB  2023-05-12 17:25:45            879         28  0.9411765        \n",
            "ursmaheshj/top-10000-popular-movies-tmdb-05-2023          Top 10000 popular Movies TMDB                   2MB  2023-05-09 13:43:53            949         33  1.0              \n",
            "gyaswanth297/world-population-insights-1970-2022          World Population Insights: 1970-2022           16KB  2023-05-16 16:25:59            872         29  0.88235295       \n",
            "sougatapramanick/happiness-index-2018-2019                Happiness Index 2018-2019                      15KB  2023-04-14 12:01:03           3809         64  1.0              \n",
            "utkarshx27/non-alcohol-fatty-liver-disease                Non-alcohol fatty liver disease (NAFLD)         3MB  2023-05-09 11:22:22            850         33  1.0              \n",
            "ahmedshahriarsakib/usa-real-estate-dataset                USA Real Estate Dataset                         2MB  2023-05-18 03:36:35           2483         58  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading dataset. \n",
        "!kaggle competitions download 'quora-question-pairs'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm1VVDnAIm-s",
        "outputId": "415b9f76-9e2a-4d7d-8f60-38c4e98f1960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading quora-question-pairs.zip to /content\n",
            " 96% 297M/309M [00:01<00:00, 252MB/s]\n",
            "100% 309M/309M [00:01<00:00, 227MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip folder. \n",
        "! unzip quora-question-pairs.zip -d '1. Dataset'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht5PKjQfLpT9",
        "outputId": "3b55b2ac-e32c-4a41-ba48-c87e02321714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  quora-question-pairs.zip\n",
            "  inflating: 1. Dataset/sample_submission.csv.zip  \n",
            "  inflating: 1. Dataset/test.csv     \n",
            "  inflating: 1. Dataset/test.csv.zip  \n",
            "  inflating: 1. Dataset/train.csv.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be importing and loading our training, and validation dataset."
      ],
      "metadata": {
        "id": "57iFDWRRHV_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking input folder. \n",
        "\n",
        "## Defining a path exploral function. \n",
        "\n",
        "def path_exploral(dir_path:str): \n",
        "    for dirname, _, filenames in os.walk(dir_path):\n",
        "        print(f\"Directory name: {dirname}\")\n",
        "        print(f\"File name: {filenames}\\n\\n\")\n",
        "    \n",
        "        for filename in filenames:\n",
        "            print(os.path.join(dirname, filename))\n",
        "\n",
        "## Checking the input folder. \n",
        "data_dir = '/content/1. Dataset'\n",
        "\n",
        "path_exploral(data_dir)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-24T13:30:51.929163Z",
          "iopub.execute_input": "2023-05-24T13:30:51.929665Z",
          "iopub.status.idle": "2023-05-24T13:30:51.940238Z",
          "shell.execute_reply.started": "2023-05-24T13:30:51.929629Z",
          "shell.execute_reply": "2023-05-24T13:30:51.939030Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVvv1IA5HV_Z",
        "outputId": "2ea89a35-337d-43bb-935f-b54f505d7ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory name: /content/1. Dataset\n",
            "File name: ['test.csv', 'test.csv.zip', 'sample_submission.csv.zip', 'train.csv.zip']\n",
            "\n",
            "\n",
            "/content/1. Dataset/test.csv\n",
            "/content/1. Dataset/test.csv.zip\n",
            "/content/1. Dataset/sample_submission.csv.zip\n",
            "/content/1. Dataset/train.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping file. \n",
        "\n",
        "## Training zip directory.\n",
        "train_zip_dir = os.path.join(data_dir,'train.csv.zip' )\n",
        "\n",
        "## Extracting training data file\n",
        "def unzip_folder(source_dir: str, destination_dir: str):\n",
        "    with ZipFile(source_dir) as zip_dir:\n",
        "        zip_dir.extractall(destination_dir)\n",
        "\n",
        "unzip_folder(train_zip_dir, data_dir)\n",
        "\n",
        "## Testing and sample directory.\n",
        "testing_zip_dir = os.path.join(data_dir, 'test.csv.zip')\n",
        "sample_zip_dir = os.path.join(data_dir, 'sample_submission.csv.zip')\n",
        "\n",
        "## Extracting testing and sample file. \n",
        "unzip_folder(testing_zip_dir, data_dir)\n",
        "unzip_folder(sample_zip_dir, data_dir)\n",
        "\n",
        "## Checking dataset folder on updated files.\n",
        "path_exploral(data_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T14:06:23.095876Z",
          "iopub.execute_input": "2023-05-24T14:06:23.096336Z",
          "iopub.status.idle": "2023-05-24T14:08:35.018616Z",
          "shell.execute_reply.started": "2023-05-24T14:06:23.096304Z",
          "shell.execute_reply": "2023-05-24T14:08:35.016989Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOE3HFGyHV_d",
        "outputId": "2c4066fe-e81e-4b1f-b170-e6b60721ada1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory name: /content/1. Dataset\n",
            "File name: ['test.csv', 'test.csv.zip', 'sample_submission.csv.zip', 'sample_submission.csv', 'train.csv', 'train.csv.zip']\n",
            "\n",
            "\n",
            "/content/1. Dataset/test.csv\n",
            "/content/1. Dataset/test.csv.zip\n",
            "/content/1. Dataset/sample_submission.csv.zip\n",
            "/content/1. Dataset/sample_submission.csv\n",
            "/content/1. Dataset/train.csv\n",
            "/content/1. Dataset/train.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting all zip files. \n",
        "!rm /content/*.zip\n",
        "!rm /content/1.\\ Dataset/*.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFLMa8CO3WA7",
        "outputId": "62e44c97-d992-40cf-f748-9102bd7ad858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/*.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking folder for update.\n",
        "path_exploral(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rr4cljG4qOF",
        "outputId": "aa70bb67-3440-4ef1-f36f-6add07bcebcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory name: /content/1. Dataset\n",
            "File name: ['test.csv', 'sample_submission.csv', 'train.csv']\n",
            "\n",
            "\n",
            "/content/1. Dataset/test.csv\n",
            "/content/1. Dataset/sample_submission.csv\n",
            "/content/1. Dataset/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset:"
      ],
      "metadata": {
        "id": "rnTxg55DHV_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Loading training and testing dataframe.\n",
        "\n",
        "## File directories. \n",
        "train_dir = os.path.join(data_dir, 'train.csv')\n",
        "test_dir = os.path.join(data_dir, 'test.csv')\n",
        "sample_sub_dir = os.paht.join(data_dir, 'sample_submission.csv')\n",
        "\n",
        "## Importing files to dataframes.\n",
        "train_df = pd.DataFrame(train_dir)\n",
        "test"
      ],
      "metadata": {
        "id": "7gbabsDe4tE-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}